# Cloud Run Functions - GCP Data Pipeline

Este repositÃ³rio contÃ©m um conjunto de **Cloud Run Functions** para processamento e ingestÃ£o de dados no Google Cloud Platform (GCP). As funÃ§Ãµes sÃ£o responsÃ¡veis por diferentes etapas do pipeline de dados, desde a ingestÃ£o de arquivos atÃ© a geraÃ§Ã£o automÃ¡tica de modelos do Dataform.

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ criacao-novas-tabelas/          # GeraÃ§Ã£o automÃ¡tica de modelos Dataform
â”œâ”€â”€ ingestao-arquivos/              # Processamento de arquivos Excel/CSV
â”œâ”€â”€ ingestao-infoprice/             # ConversÃ£o de arquivos InfoPrice
â”œâ”€â”€ ingestao-kruzer-produtos-pbm/   # IngestÃ£o de dados PBM via API
â””â”€â”€ README.md
```

## ğŸš€ Cloud Run Functions

### 1. **CriaÃ§Ã£o de Novas Tabelas** (`criacao-novas-tabelas/`)

**FunÃ§Ã£o**: GeraÃ§Ã£o automÃ¡tica de modelos Dataform para novas tabelas

**DescriÃ§Ã£o**: 
- Consulta tabelas pendentes no BigQuery
- Gera automaticamente arquivos `.sqlx` e `.js` do Dataform
- Cria branches no GitHub com as definiÃ§Ãµes
- Suporta migraÃ§Ãµes INCREMENTAL e FULL
- Configura partiÃ§Ãµes BigQuery automaticamente para tabelas incrementais

**Tecnologias**:
- Google Cloud BigQuery
- GitHub API
- SQLAlchemy (conexÃ£o com bancos de origem)
- Dataform

**VariÃ¡veis de Ambiente**:
```bash
GCP_PROJECT_ID=seu-projeto-gcp
GITHUB_TOKEN_SECRET_ID=github-token-secret
GITHUB_USER=usuario-github
GITHUB_REPO=repositorio-dataform
```

**Payload de Entrada**:
```json
{
  "config_table_id": "projeto.dataset.tabela_config"
}
```

---

### 2. **IngestÃ£o de Arquivos** (`ingestao-arquivos/`)

**FunÃ§Ã£o**: Processamento de arquivos Excel/CSV de compartilhamentos SMB

**DescriÃ§Ã£o**:
- Conecta a compartilhamentos SMB para buscar arquivos
- Processa diferentes tipos de arquivos (Excel, CSV, TXT)
- Aplica transformaÃ§Ãµes especÃ­ficas por tipo de arquivo
- Salva dados processados no Cloud Storage como Parquet
- Suporta modos de escrita: `overwrite` e `partitioned`

**Arquivos Suportados**:
- `Ãrvore MKT - Servicos.xlsx`
- `Info Lojas.xlsx`
- `Darks_Store_Lojas.xlsx`
- `Info Lojas Servicos Farmaceuticos.xlsx`
- `PRICEPOINT_CADASTRO PRODUTO_v2.xlsx`
- `Canal de Vendas.xlsx`
- `Redes_InfoPrice.xlsx`
- `Bairros_InfoPrice.xlsx`
- Arquivos IQVIA (`.txt`)

**Tecnologias**:
- Google Cloud Storage
- SMB Protocol
- Pandas
- PyArrow

**VariÃ¡veis de Ambiente**:
```bash
GCP_PROJECT=seu-projeto-gcp
PROCESSED_BUCKET=bucket-processado
SMB_SERVER_IP=10.0.1.100
SMB_SHARE_PATH=Arquivos Suporte PBI
FILE_TO_PROCESS=nome-do-arquivo.xlsx
```

---

### 3. **IngestÃ£o InfoPrice** (`ingestao-infoprice/`)

**FunÃ§Ã£o**: ConversÃ£o de arquivos InfoPrice de GZ para Parquet

**DescriÃ§Ã£o**:
- Processa arquivos `.gz` do Cloud Storage
- Converte dados para formato Parquet
- Aplica schema do BigQuery automaticamente
- Suporta triggers do GCS e execuÃ§Ã£o manual via Airflow

**Tecnologias**:
- Google Cloud Storage
- Google Cloud BigQuery
- Pandas
- PyArrow

**VariÃ¡veis de Ambiente**:
```bash
BUCKET_NAME=bucket-origem
PREFIX=prefixo-arquivos
PROJECT_ID=seu-projeto-gcp
DATASET_BQ=dataset-destino
TABELA_BQ=tabela-destino
RUN=2024-01-01  # Opcional, para execuÃ§Ã£o manual
```

**Triggers**:
- Cloud Storage (quando arquivo `.gz` Ã© carregado)
- Airflow (execuÃ§Ã£o manual com parÃ¢metros)

---

### 4. **IngestÃ£o Kruzer Produtos PBM** (`ingestao-kruzer-produtos-pbm/`)

**FunÃ§Ã£o**: IngestÃ£o de dados PBM via API HTTP

**DescriÃ§Ã£o**:
- Consulta API HTTP para obter dados PBM
- Aplica transformaÃ§Ãµes especÃ­ficas do domÃ­nio
- Salva dados como Parquet no Cloud Storage
- Implementa logging estruturado com Cloud Logging

**Tecnologias**:
- Requests (HTTP)
- Pandas
- PyArrow
- Google Cloud Storage
- Google Cloud Logging

**VariÃ¡veis de Ambiente**:
```bash
GCP_PROJECT=seu-projeto-gcp
BUCKET_NAME=bucket-destino
API_URL=https://api.exemplo.com/dados
FILE_NAME=nome-arquivo-saida
```

---

## ğŸ› ï¸ ConfiguraÃ§Ã£o e Deploy

### PrÃ©-requisitos

1. **Google Cloud SDK** instalado e configurado
2. **Docker** para build das imagens
3. **PermissÃµes IAM** adequadas no projeto GCP
4. **Secrets** configurados no Secret Manager

### Deploy das FunÃ§Ãµes

```bash
# Para cada funÃ§Ã£o, navegue atÃ© o diretÃ³rio e execute:

# 1. CriaÃ§Ã£o de Novas Tabelas
cd criacao-novas-tabelas
gcloud run deploy criacao-novas-tabelas \
  --source . \
  --platform managed \
  --region us-central1 \
  --memory 2Gi \
  --timeout 900 \
  --set-env-vars GCP_PROJECT_ID=seu-projeto

# 2. IngestÃ£o de Arquivos
cd ../ingestao-arquivos
gcloud run deploy ingestao-arquivos \
  --source . \
  --platform managed \
  --region us-central1 \
  --memory 1Gi \
  --timeout 600

# 3. IngestÃ£o InfoPrice
cd ../ingestao-infoprice
gcloud run deploy ingestao-infoprice \
  --source . \
  --platform managed \
  --region us-central1 \
  --memory 1Gi \
  --timeout 600

# 4. IngestÃ£o Kruzer PBM
cd ../ingestao-kruzer-produtos-pbm
gcloud run deploy ingestao-kruzer-pbm \
  --source . \
  --platform managed \
  --region us-central1 \
  --memory 512Mi \
  --timeout 300
```

### ConfiguraÃ§Ã£o de Secrets

```bash
# GitHub Token
gcloud secrets create github-token --data-file=github-token.txt

# Credenciais SMB
gcloud secrets create admin-bi-user --data-file=smb-user.txt
gcloud secrets create admin-bi-password --data-file=smb-password.txt

# Outros secrets conforme necessÃ¡rio
```

## ğŸ“Š Fluxo de Dados

```mermaid
graph TD
    A[Arquivos SMB] --> B[ingestao-arquivos]
    B --> C[Cloud Storage - Parquet]
    
    D[API PBM] --> E[ingestao-kruzer-produtos-pbm]
    E --> C
    
    F[Arquivos .gz] --> G[ingestao-infoprice]
    G --> C
    
    C --> H[BigQuery External Tables]
    H --> I[criacao-novas-tabelas]
    I --> J[Dataform Models]
    J --> K[BigQuery Tables]
```

## ğŸ”§ Desenvolvimento Local

### Setup do Ambiente

```bash
# Clone o repositÃ³rio
git clone <repo-url>
cd cloud-run-functions

# Para cada funÃ§Ã£o, instale as dependÃªncias
cd criacao-novas-tabelas
pip install -r requirements.txt

# Configure as variÃ¡veis de ambiente
cp .env.example .env
# Edite o .env com suas configuraÃ§Ãµes
```

### Testes Locais

```bash
# Teste individual de cada funÃ§Ã£o
cd criacao-novas-tabelas
python main.py

# Ou usando o Functions Framework
functions-framework --target=main --source=main.py --port=8080
```

## ğŸ“ Logs e Monitoramento

- **Cloud Logging**: Todas as funÃ§Ãµes utilizam Cloud Logging para logs estruturados
- **Cloud Monitoring**: MÃ©tricas automÃ¡ticas de execuÃ§Ã£o, latÃªncia e erros
- **Error Reporting**: Captura automÃ¡tica de exceÃ§Ãµes nÃ£o tratadas

## ğŸ”’ SeguranÃ§a

- **Secret Manager**: Credenciais sensÃ­veis armazenadas de forma segura
- **IAM**: PermissÃµes mÃ­nimas necessÃ¡rias para cada funÃ§Ã£o
- **VPC**: ConexÃµes seguras com recursos internos quando necessÃ¡rio
- **Audit Logs**: Logs de auditoria habilitados para todas as operaÃ§Ãµes

## ğŸ“ˆ Escalabilidade

- **Auto-scaling**: Cloud Run escala automaticamente baseado na demanda
- **Concurrency**: ConfiguraÃ§Ã£o de concorrÃªncia por instÃ¢ncia
- **Memory/CPU**: Recursos ajustÃ¡veis por funÃ§Ã£o conforme necessidade
- **Timeout**: Timeouts configurados adequadamente para cada tipo de processamento

## ğŸš¨ Troubleshooting

### Problemas Comuns

1. **Timeout de execuÃ§Ã£o**: Aumente o timeout da funÃ§Ã£o
2. **MemÃ³ria insuficiente**: Ajuste a memÃ³ria alocada
3. **Erro de permissÃµes**: Verifique as permissÃµes IAM
4. **Falha na conexÃ£o SMB**: Verifique credenciais e conectividade de rede

### Logs Ãšteis

```bash
# Visualizar logs de uma funÃ§Ã£o especÃ­fica
gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=ingestao-arquivos" --limit=50

# Logs de erro
gcloud logging read "severity>=ERROR" --limit=20
```

## ğŸ“ Suporte

Para questÃµes tÃ©cnicas ou problemas com as funÃ§Ãµes, consulte:
- Logs no Cloud Logging
- MÃ©tricas no Cloud Monitoring
- DocumentaÃ§Ã£o do Google Cloud Run
- Issues do repositÃ³rio

---

**Ãšltima atualizaÃ§Ã£o**: Janeiro 2024  
**VersÃ£o**: 1.0.0
