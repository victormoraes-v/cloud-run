# Cloud Run Functions - GCP Data Pipeline

Este reposit√≥rio cont√©m um conjunto de **Cloud Run Services e Jobs** para processamento e ingest√£o de dados no Google Cloud Platform (GCP). As fun√ß√µes s√£o respons√°veis por diferentes etapas do pipeline de dados, desde a ingest√£o de arquivos at√© a gera√ß√£o autom√°tica de modelos do Dataform.

## üìÅ Estrutura do Projeto

```
‚îú‚îÄ‚îÄ criacao-novas-tabelas/          # Cloud Run Service - Gera√ß√£o autom√°tica de modelos Dataform
‚îú‚îÄ‚îÄ ingestao-arquivos/              # Cloud Run Job - Processamento de arquivos Excel/CSV
‚îú‚îÄ‚îÄ ingestao-infoprice/             # Cloud Run Job - Convers√£o de arquivos InfoPrice
‚îú‚îÄ‚îÄ ingestao-kruzer-produtos-pbm/   # Cloud Run Job - Ingest√£o de dados PBM via API
‚îú‚îÄ‚îÄ ingestao-precifica/             # Cloud Run Job - Ingest√£o de dados Precifica via API
‚îî‚îÄ‚îÄ README.md
```

## üöÄ Cloud Run Services e Jobs

### 1. **Cria√ß√£o de Novas Tabelas** (`criacao-novas-tabelas/`)

**Tipo**: Cloud Run Service (HTTP)

**Fun√ß√£o**: Gera√ß√£o autom√°tica de modelos Dataform para novas tabelas

**Descri√ß√£o**: 
- Consulta tabelas pendentes no BigQuery
- Gera automaticamente arquivos `.sqlx` e `.js` do Dataform
- Cria branches no GitHub com as defini√ß√µes
- Suporta migra√ß√µes INCREMENTAL e FULL
- Configura parti√ß√µes BigQuery automaticamente para tabelas incrementais

**Tecnologias**:
- Google Cloud BigQuery
- GitHub API
- SQLAlchemy (conex√£o com bancos de origem)
- Dataform

**Vari√°veis de Ambiente**:
```bash
GCP_PROJECT_ID=seu-projeto-gcp
GITHUB_TOKEN_SECRET_ID=github-token-secret
GITHUB_USER=usuario-github
GITHUB_REPO=repositorio-dataform
```

**Payload de Entrada**:
```json
{
  "config_table_id": "projeto.dataset.tabela_config"
}
```

---

### 2. **Ingest√£o de Arquivos** (`ingestao-arquivos/`)

**Tipo**: Cloud Run Job

**Fun√ß√£o**: Processamento de arquivos Excel/CSV de compartilhamentos SMB

**Descri√ß√£o**:
- Conecta a compartilhamentos SMB para buscar arquivos
- Processa diferentes tipos de arquivos (Excel, CSV, TXT)
- Aplica transforma√ß√µes espec√≠ficas por tipo de arquivo
- Salva dados processados no Cloud Storage como Parquet
- Suporta modos de escrita: `overwrite` e `partitioned`

**Arquivos Suportados**:
- `√Årvore MKT - Servicos.xlsx`
- `Info Lojas.xlsx`
- `Darks_Store_Lojas.xlsx`
- `Info Lojas Servicos Farmaceuticos.xlsx`
- `PRICEPOINT_CADASTRO PRODUTO_v2.xlsx`
- `Canal de Vendas.xlsx`
- `Redes_InfoPrice.xlsx`
- `Bairros_InfoPrice.xlsx`
- `Agenda_Sugestao_Compras.xlsx`
- `Expurgo_Pedidos_Compras.xlsx`
- `Expurgo_Mapas.xlsx`
- Arquivos IQVIA (`.txt`)

**Tecnologias**:
- Google Cloud Storage
- SMB Protocol
- Pandas
- PyArrow

**Vari√°veis de Ambiente**:
```bash
GCP_PROJECT=seu-projeto-gcp
PROCESSED_BUCKET=bucket-processado
SMB_SERVER_IP=10.0.1.100
SMB_SHARE_PATH=Arquivos Suporte PBI
FILE_TO_PROCESS=nome-do-arquivo.xlsx
```

---

### 3. **Ingest√£o InfoPrice** (`ingestao-infoprice/`)

**Tipo**: Cloud Run Job

**Fun√ß√£o**: Convers√£o de arquivos InfoPrice de GZ para Parquet

**Descri√ß√£o**:
- Processa arquivos `.gz` do Cloud Storage
- Converte dados para formato Parquet
- Aplica schema do BigQuery automaticamente
- Suporta triggers do GCS e execu√ß√£o manual via Airflow

**Tecnologias**:
- Google Cloud Storage
- Google Cloud BigQuery
- Pandas
- PyArrow

**Vari√°veis de Ambiente**:
```bash
BUCKET_NAME=bucket-origem
PREFIX=prefixo-arquivos
PROJECT_ID=seu-projeto-gcp
DATASET_BQ=dataset-destino
TABELA_BQ=tabela-destino
RUN=2024-01-01  # Opcional, para execu√ß√£o manual
```

**Triggers**:
- Cloud Storage (quando arquivo `.gz` √© carregado)
- Airflow (execu√ß√£o manual com par√¢metros)

---

### 4. **Ingest√£o Kruzer Produtos PBM** (`ingestao-kruzer-produtos-pbm/`)

**Tipo**: Cloud Run Job

**Fun√ß√£o**: Ingest√£o de dados PBM via API HTTP

**Descri√ß√£o**:
- Consulta API HTTP para obter dados PBM
- Aplica transforma√ß√µes espec√≠ficas do dom√≠nio
- Salva dados como Parquet no Cloud Storage
- Implementa logging estruturado com Cloud Logging

**Tecnologias**:
- Requests (HTTP)
- Pandas
- PyArrow
- Google Cloud Storage
- Google Cloud Logging

**Vari√°veis de Ambiente**:
```bash
GCP_PROJECT=seu-projeto-gcp
BUCKET_NAME=bucket-destino
API_URL=https://api.exemplo.com/dados
FILE_NAME=nome-arquivo-saida
```

---

### 5. **Ingest√£o Precifica** (`ingestao-precifica/`)

**Tipo**: Cloud Run Job

**Fun√ß√£o**: Ingest√£o de dados Precifica via API HTTP

**Descri√ß√£o**:
- Consulta API HTTP para obter dados de produtos Precifica
- Processa dados de forma concorrente (m√∫ltiplas requisi√ß√µes simult√¢neas)
- Aplica transforma√ß√µes e normaliza√ß√µes espec√≠ficas do dom√≠nio
- Adiciona colunas calculadas
- Salva dados como CSV no Cloud Storage

**Tecnologias**:
- Requests (HTTP)
- Pandas
- Google Cloud Storage
- ConfigParser (configura√ß√£o)

**Vari√°veis de Ambiente**:
```bash
GCP_PROJECT=seu-projeto-gcp
API_BASE_URL=https://api.precifica.com
API_CLIENT_KEY=client-key
API_SECRET_KEY=secret-key
API_PLATAFORMA=plataforma
API_DOMINIO=dominio
GCS_BUCKET=bucket-destino
GCS_PREFIX=raw/precifica/
# Opcional: usar Secret Manager
GCP_SECRET_NAME=nome-do-secret
```

---

## üõ†Ô∏è Build e Deploy

### Pr√©-requisitos

1. **Google Cloud SDK** instalado e configurado
2. **Docker** para build das imagens
3. **Permiss√µes IAM** adequadas no projeto GCP
4. **Secrets** configurados no Secret Manager
5. **Artifact Registry** ou **Container Registry** configurado

### Build e Push de Imagens Docker

Para cada projeto, voc√™ precisa fazer o build da imagem Docker e fazer push para o Artifact Registry/Container Registry:

```bash
# Configurar vari√°veis comuns
export PROJECT_ID=seu-projeto-gcp
export REGION=us-central1
export REPOSITORY=cloud-run-images  # Nome do reposit√≥rio no Artifact Registry

# 1. Cria√ß√£o de Novas Tabelas
cd criacao-novas-tabelas
docker build -t gcr.io/${PROJECT_ID}/criacao-novas-tabelas:latest .
docker push gcr.io/${PROJECT_ID}/criacao-novas-tabelas:latest
# Ou usando Artifact Registry:
# docker build -t ${REGION}-docker.pkg.dev/${PROJECT_ID}/${REPOSITORY}/criacao-novas-tabelas:latest .
# docker push ${REGION}-docker.pkg.dev/${PROJECT_ID}/${REPOSITORY}/criacao-novas-tabelas:latest

# 2. Ingest√£o de Arquivos
cd ../ingestao-arquivos
docker build -t gcr.io/${PROJECT_ID}/ingestao-arquivos:latest .
docker push gcr.io/${PROJECT_ID}/ingestao-arquivos:latest

# 3. Ingest√£o InfoPrice
cd ../ingestao-infoprice
docker build -t gcr.io/${PROJECT_ID}/ingestao-infoprice:latest .
docker push gcr.io/${PROJECT_ID}/ingestao-infoprice:latest

# 4. Ingest√£o Kruzer Produtos PBM
cd ../ingestao-kruzer-produtos-pbm
docker build -t gcr.io/${PROJECT_ID}/ingestao-kruzer-produtos-pbm:latest .
docker push gcr.io/${PROJECT_ID}/ingestao-kruzer-produtos-pbm:latest

# 5. Ingest√£o Precifica
cd ../ingestao-precifica
docker build -t gcr.io/${PROJECT_ID}/ingestao-precifica:latest .
docker push gcr.io/${PROJECT_ID}/ingestao-precifica:latest
```

### Deploy/Atualiza√ß√£o de Cloud Run Service

**Cria√ß√£o de Novas Tabelas** (Cloud Run Service):

```bash
cd criacao-novas-tabelas
gcloud run deploy criacao-novas-tabelas \
  --image gcr.io/${PROJECT_ID}/criacao-novas-tabelas:latest \
  --platform managed \
  --region us-central1 \
  --memory 2Gi \
  --timeout 900 \
  --set-env-vars GCP_PROJECT_ID=${PROJECT_ID} \
  --set-secrets GITHUB_TOKEN_SECRET_ID=github-token:latest
```

### Deploy/Atualiza√ß√£o de Cloud Run Jobs

**Ingest√£o de Arquivos**:

```bash
cd ingestao-arquivos
gcloud run jobs update ingestao-arquivos \
  --image gcr.io/${PROJECT_ID}/ingestao-arquivos:latest \
  --region us-central1 \
  --memory 1Gi \
  --timeout 600 \
  --set-env-vars GCP_PROJECT=${PROJECT_ID},PROCESSED_BUCKET=bucket-processado,SMB_SERVER_IP=10.0.1.100,SMB_SHARE_PATH="Arquivos Suporte PBI" \
  --set-secrets SMB_USER=admin-bi-user:latest,SMB_PASSWORD=admin-bi-password:latest \
  --max-retries 1
```

**Ingest√£o InfoPrice**:

```bash
cd ingestao-infoprice
gcloud run jobs update ingestao-infoprice \
  --image gcr.io/${PROJECT_ID}/ingestao-infoprice:latest \
  --region us-central1 \
  --memory 1Gi \
  --timeout 600 \
  --set-env-vars BUCKET_NAME=bucket-origem,PREFIX=prefixo-arquivos,PROJECT_ID=${PROJECT_ID},DATASET_BQ=dataset-destino,TABELA_BQ=tabela-destino \
  --max-retries 1
```

**Ingest√£o Kruzer Produtos PBM**:

```bash
cd ingestao-kruzer-produtos-pbm
gcloud run jobs update ingestao-kruzer-produtos-pbm \
  --image gcr.io/${PROJECT_ID}/ingestao-kruzer-produtos-pbm:latest \
  --region us-central1 \
  --memory 512Mi \
  --timeout 300 \
  --set-env-vars GCP_PROJECT=${PROJECT_ID},BUCKET_NAME=bucket-destino,API_URL=https://api.exemplo.com/dados,FILE_NAME=nome-arquivo-saida \
  --max-retries 1
```

**Ingest√£o Precifica**:

```bash
cd ingestao-precifica
gcloud run jobs update ingestao-precifica \
  --image gcr.io/${PROJECT_ID}/ingestao-precifica:latest \
  --region us-central1 \
  --memory 1Gi \
  --timeout 600 \
  --set-env-vars GCP_PROJECT=${PROJECT_ID},GCS_BUCKET=bucket-destino,GCS_PREFIX=raw/precifica/ \
  --set-secrets API_CLIENT_KEY=precifica-client-key:latest,API_SECRET_KEY=precifica-secret-key:latest \
  --max-retries 1
```

**Nota**: Se for a primeira vez criando o job, use `gcloud run jobs create` ao inv√©s de `update`.

## üöÄ Executar Cloud Run Jobs

### Chamar Cloud Run Jobs

Para executar um Cloud Run Job, voc√™ pode usar o comando `gcloud run jobs execute`:

```bash
# Executar Ingest√£o de Arquivos
gcloud run jobs execute ingestao-arquivos \
  --region us-central1 \
  --update-env-vars FILE_TO_PROCESS=nome-do-arquivo.xlsx

# Executar Ingest√£o InfoPrice
gcloud run jobs execute ingestao-infoprice \
  --region us-central1 \
  --update-env-vars RUN=2024-01-01

# Executar Ingest√£o Kruzer Produtos PBM
gcloud run jobs execute ingestao-kruzer-produtos-pbm \
  --region us-central1

# Executar Ingest√£o Precifica
gcloud run jobs execute ingestao-precifica \
  --region us-central1
```

### Chamar Cloud Run Service (HTTP)

Para chamar o Cloud Run Service **Cria√ß√£o de Novas Tabelas**, fa√ßa uma requisi√ß√£o HTTP:

```bash
# Obter a URL do servi√ßo
SERVICE_URL=$(gcloud run services describe criacao-novas-tabelas \
  --region us-central1 \
  --format 'value(status.url)')

# Fazer requisi√ß√£o POST
curl -X POST ${SERVICE_URL} \
  -H "Authorization: Bearer $(gcloud auth print-identity-token)" \
  -H "Content-Type: application/json" \
  -d '{"config_table_id": "projeto.dataset.tabela_config"}'
```

Ou usando o gcloud:

```bash
gcloud run services call criacao-novas-tabelas \
  --region us-central1 \
  --data '{"config_table_id": "projeto.dataset.tabela_config"}'
```

### Monitorar Execu√ß√£o de Jobs

```bash
# Listar execu√ß√µes de um job
gcloud run jobs executions list \
  --job ingestao-arquivos \
  --region us-central1

# Ver logs de uma execu√ß√£o espec√≠fica
gcloud run jobs executions describe EXECUTION_NAME \
  --job ingestao-arquivos \
  --region us-central1

# Ver logs em tempo real
gcloud logging tail "resource.type=cloud_run_job AND resource.labels.job_name=ingestao-arquivos"
```

## üîê Configura√ß√£o de Secrets

```bash
# GitHub Token
gcloud secrets create github-token --data-file=github-token.txt

# Credenciais SMB
gcloud secrets create admin-bi-user --data-file=smb-user.txt
gcloud secrets create admin-bi-password --data-file=smb-password.txt

# Outros secrets conforme necess√°rio
```

## üìä Fluxo de Dados

```mermaid
graph TD
    A[Arquivos SMB] --> B[ingestao-arquivos Job]
    B --> C[Cloud Storage - Parquet]
    
    D[API PBM] --> E[ingestao-kruzer-produtos-pbm Job]
    E --> C
    
    F[Arquivos .gz] --> G[ingestao-infoprice Job]
    G --> C
    
    H[API Precifica] --> I[ingestao-precifica Job]
    I --> J[Cloud Storage - CSV]
    
    C --> K[BigQuery External Tables]
    K --> L[criacao-novas-tabelas Service]
    L --> M[Dataform Models]
    M --> N[BigQuery Tables]
```

## üîß Desenvolvimento Local

### Pr√©-requisitos para Teste Local

1. **Python 3.10+** instalado
2. **Virtual Environment** (venv) configurado
3. **Google Cloud SDK** instalado e autenticado (`gcloud auth application-default login`)
4. **Vari√°veis de ambiente** configuradas (via `.env` ou export)
5. **Secrets** do GCP acess√≠veis (via Application Default Credentials)

### Testes Locais

#### 1. Cria√ß√£o de Novas Tabelas

```bash
# Entrar na pasta do projeto
cd criacao-novas-tabelas

# Criar e ativar venv (se ainda n√£o tiver)
python -m venv venv
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Instalar depend√™ncias
pip install -r requirements.txt

# Configurar vari√°veis de ambiente (criar arquivo .env ou export)
export GCP_PROJECT_ID=seu-projeto-gcp
export GITHUB_TOKEN_SECRET_ID=github-token-secret
export GITHUB_USER=usuario-github
export GITHUB_REPO=repositorio-dataform

# Executar localmente (simula requisi√ß√£o HTTP)
python main.py
# Ou usando Functions Framework para testar como HTTP service:
functions-framework --target=main --port=8080
# Em outro terminal, fazer requisi√ß√£o:
curl -X POST http://localhost:8080 \
  -H "Content-Type: application/json" \
  -d '{"config_table_id": "projeto.dataset.tabela_config"}'
```

#### 2. Ingest√£o de Arquivos

```bash
# Entrar na pasta do projeto
cd ingestao-arquivos

# Criar e ativar venv
python -m venv venv
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Instalar depend√™ncias
pip install -r requirements.txt

# Configurar vari√°veis de ambiente
export GCP_PROJECT=seu-projeto-gcp
export PROCESSED_BUCKET=bucket-processado
export SMB_SERVER_IP=10.0.1.100
export SMB_SHARE_PATH=Arquivos Suporte PBI
export FILE_TO_PROCESS=nome-do-arquivo.xlsx

# Executar localmente
python main.py
```

#### 3. Ingest√£o InfoPrice

```bash
# Entrar na pasta do projeto
cd ingestao-infoprice

# Criar e ativar venv
python -m venv venv
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Instalar depend√™ncias
pip install -r requirements.txt

# Configurar vari√°veis de ambiente
export BUCKET_NAME=bucket-origem
export PREFIX=prefixo-arquivos
export PROJECT_ID=seu-projeto-gcp
export DATASET_BQ=dataset-destino
export TABELA_BQ=tabela-destino
export RUN=2024-01-01  # Opcional

# Executar localmente
python main.py
```

#### 4. Ingest√£o Kruzer Produtos PBM

```bash
# Entrar na pasta do projeto
cd ingestao-kruzer-produtos-pbm

# Criar e ativar venv
python -m venv venv
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Instalar depend√™ncias
pip install -r requirements.txt

# Configurar vari√°veis de ambiente
export GCP_PROJECT=seu-projeto-gcp
export BUCKET_NAME=bucket-destino
export API_URL=https://api.exemplo.com/dados
export FILE_NAME=nome-arquivo-saida

# Executar localmente
python main.py
```

#### 5. Ingest√£o Precifica

```bash
# Entrar na pasta do projeto
cd ingestao-precifica

# Criar e ativar venv
python -m venv venv
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Instalar depend√™ncias
pip install -r requirements.txt

# Configurar vari√°veis de ambiente
export GCP_PROJECT=seu-projeto-gcp
export API_BASE_URL=https://api.precifica.com
export API_CLIENT_KEY=client-key
export API_SECRET_KEY=secret-key
export API_PLATAFORMA=plataforma
export API_DOMINIO=dominio
export GCS_BUCKET=bucket-destino
export GCS_PREFIX=raw/precifica/

# Executar localmente
python main.py
```

## üìù Logs e Monitoramento

- **Cloud Logging**: Todas as fun√ß√µes utilizam Cloud Logging para logs estruturados
- **Cloud Monitoring**: M√©tricas autom√°ticas de execu√ß√£o, lat√™ncia e erros
- **Error Reporting**: Captura autom√°tica de exce√ß√µes n√£o tratadas

## üîí Seguran√ßa

- **Secret Manager**: Credenciais sens√≠veis armazenadas de forma segura
- **IAM**: Permiss√µes m√≠nimas necess√°rias para cada fun√ß√£o
- **VPC**: Conex√µes seguras com recursos internos quando necess√°rio
- **Audit Logs**: Logs de auditoria habilitados para todas as opera√ß√µes

## üìà Escalabilidade

- **Auto-scaling**: Cloud Run escala automaticamente baseado na demanda
- **Concurrency**: Configura√ß√£o de concorr√™ncia por inst√¢ncia
- **Memory/CPU**: Recursos ajust√°veis por fun√ß√£o conforme necessidade
- **Timeout**: Timeouts configurados adequadamente para cada tipo de processamento

## üö® Troubleshooting

### Problemas Comuns

1. **Timeout de execu√ß√£o**: Aumente o timeout da fun√ß√£o
2. **Mem√≥ria insuficiente**: Ajuste a mem√≥ria alocada
3. **Erro de permiss√µes**: Verifique as permiss√µes IAM
4. **Falha na conex√£o SMB**: Verifique credenciais e conectividade de rede

### Logs √öteis

```bash
# Visualizar logs de uma fun√ß√£o espec√≠fica
gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=ingestao-arquivos" --limit=50

# Logs de erro
gcloud logging read "severity>=ERROR" --limit=20
```

## üìû Suporte

Para quest√µes t√©cnicas ou problemas com as fun√ß√µes, consulte:
- Logs no Cloud Logging
- M√©tricas no Cloud Monitoring
- Documenta√ß√£o do Google Cloud Run
- Issues do reposit√≥rio
